{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 0:: Playing the Slots (with Posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This lab is not assessed.*\n",
    "\n",
    "## Background\n",
    "\n",
    "This coursework provides an exercise in computing posterior distributions, in the context of trying to maximise the \"payout\" of a \"multi-armed bandit\". (You may see this application again in one or more other units - it's related to problems in reinforcement learning.)\n",
    "\n",
    "A $K$-armed bandit (a little like a slot machine) has $K$ levers, and you may choose which one to pull. Associated with each lever is some payout distribution, the parameters of which are unknown to you at the outset. The name of the \"game\" is to try and maximise your earnings across a fixed number of $N$ pulls (here, we try $N=100$).\n",
    " \n",
    "A key question to consider at each pull is whether to \"exploit\" or \"explore\":\n",
    "- **exploit:** choose to pull the lever that has proved most profitable so far, or\n",
    "- **explore:** gather data from another lever and improve the state of knowledge.\n",
    "\n",
    "Managing the \"exploration-exploitation\" trade-off is a key aspect in search and reinforcement learning. This lab will utilise a Bayesian approach (which isn't necessary the best to adopt, but demonstrates the principles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Code\n",
    "\n",
    "First, download (from Moodle) and import the library \"cm50268_bandit.py\" which defines some K-armed bandits. This lab will focus on two of them:\n",
    "\n",
    "- `BernoulliBandit` which pays out either £25 with unknown arm-dependent probability $\\theta_k$, and nothing otherwise,\n",
    "- `GammaBandit` which pays out a positive amount based on a Gamma distribution with fixed shape parameter $\\alpha=5$ but unknown arm-dependent rate parameter $\\beta_k$.\n",
    "\n",
    "All bandits have a fixed cost of £10 to pull a lever. This dosen't affect strategy, but makes the earnings charts slightly more \"realistic\".\n",
    "\n",
    "As well as the bandit definitions (imported), there is one piece of further code which is discussed shortly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import the usual libraries, including scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Import the bandit classes\n",
    "import cm50268_bandit as lab0\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our algorithm\n",
    "\n",
    "Second, we define below a function which runs the slot machine for a given number of pulls (default is 100). To utilise it, as well as one of the pre-defined \"bandit\" class instances, you also pass it a sub-class instance of a `Selector` (an example is given shortly), the base class for which is at the end of the \"cm50268_bandit.py\" library.\n",
    "\n",
    "This derived class must define three methods: `lever_select()`, `update_state(lever, payback)` and `reset_state()`:\n",
    "\n",
    "- `lever_select()` is your method to choose which lever to pull, and should return an integer from 0 to K-1\n",
    "- `update_state(lever, payback)` is your method to update any internal memory, or state, as the result of receiving `payback` for pulling arm `lever`\n",
    "- `reset_state()` is to initialise/reset the internal state (for the purposes of repeated assessments)\n",
    "\n",
    "The class, of course, should also declare any internal state in its constructor.\n",
    "\n",
    "The `run_selector(bandit, selector)` function below returns an array of size $N$ (default 100) containing the cumulative earnings at each point in time for `selector` playing on `bandit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# run_selector\n",
    "#\n",
    "# A function to test out a lever selection algorithm\n",
    "#\n",
    "cost = 10  # Adds realism, but doesn't affect strategy\n",
    "#\n",
    "def run_selector(bandit, selector, N=100):\n",
    "    earnings = 0\n",
    "    earnings_log = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        # select-pull-update cycle\n",
    "        lever = selector.lever_select()\n",
    "        payout = bandit.pull(lever)\n",
    "        selector.update_state(lever, payout)\n",
    "        #\n",
    "        earnings += (payout - cost)\n",
    "        earnings_log[n] = earnings\n",
    "    #\n",
    "    return earnings_log\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example (Purely Random) Selector Algorithm: Exploration Only\n",
    "Next we define a sample class, `RandomSelector`. It is state-less (has no memory) and very simple: it learns nothing from the outcomes of its strategy. It simply chooses a lever at random every time; in other words, it only \"explores\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSelector(lab0.Selector):\n",
    "\n",
    "    def __init__(self, K):\n",
    "        super().__init__(K)  # Does nothing other than set self.K = K\n",
    "        # No state\n",
    "        \n",
    "    def reset_state(self):\n",
    "        # There is no state\n",
    "        pass\n",
    "\n",
    "    def update_state(self, lever, payout):\n",
    "        # Nothing to do here\n",
    "        pass\n",
    "\n",
    "    def lever_select(self):\n",
    "        #\n",
    "        # Choose one purely at random\n",
    "        # \n",
    "        lever = np.random.randint(self.K)\n",
    "        return lever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Earnings Performance\n",
    "\n",
    "We now define a useful function `plot_performance()` for visualising the outcome of running our selection algorithms on any particular bandit. As well as graphing the cumulative earnings over time, it also plots (in grey dashes) the *expected* average earnings (if you chose levers at random) and the (higher) *expected* maximum earnings (if you had perfect knowledge and chose the best lever every time). Remember that these are \"expected\" earnings - even if you pull the best lever each time, the payback is still a random variable.\n",
    "\n",
    "The function takes a `Reps` argument, which instructs it to average the earnings over that number of runs. It defaults to one, but we set `Reps=100` below when assessing overall performance (to average out the \"lucky streaks\"). It may take a few seconds to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_performance(bandit, selectors_list, N=100, Reps=1, title=None, legend=None):\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    final_scores = []\n",
    "    for selector in selectors_list:\n",
    "        mean_earnings = np.zeros(N)\n",
    "        bandit_mean = 0\n",
    "        bandit_max = 0\n",
    "        for run in range(Reps):\n",
    "            #\n",
    "            bandit.reset(run)\n",
    "            selector.reset_state()\n",
    "            #\n",
    "            earn_log = run_selector(bandit, selector, N)\n",
    "            mean_earnings += earn_log\n",
    "            bandit_mean += bandit.mean_return()-cost\n",
    "            bandit_max += bandit.max_return()-cost\n",
    "        mean_earnings /= Reps\n",
    "        plt.plot(range(N), mean_earnings, '-')\n",
    "        final_scores.append(mean_earnings[-1])\n",
    "    #\n",
    "    if legend:\n",
    "        print(\"Final Earnings\\n==============\")\n",
    "        for idx, score in enumerate(final_scores):\n",
    "            print(\"£{0:.2f} <- {1}\".format(score, legend[idx]))\n",
    "    #\n",
    "    # Plot theoretical mean (random choice) and maximum (clairvoyant) return\n",
    "    #\n",
    "    plt.plot([0, N], [0, N*bandit_mean/Reps], '--', color='silver')\n",
    "    plt.plot([0, N], [0, N*bandit_max/Reps], '--', color='silver')\n",
    "    #\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Earnings')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if legend:\n",
    "        plt.legend(legend)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Selector Performance\n",
    "\n",
    "We experiment with a `BernoulliBandit` first, with $K=6$ levers (you can of course experiment).\n",
    "\n",
    "The code below applies the `plot_performance()` function  to assess the `RandomSelector` scheme. The earnings of the random selector should roughly coincide with the expected average line (if averaged over sufficient repetitions). The default number of repetions here is 100, although 500-1000 would give smoother earnings curves (with more waiting time ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test the random selector ####\n",
    "\n",
    "# Define bandit with given random seed and set cost\n",
    "#\n",
    "cost = 10 # Doesn't affect the strategy, but adds \"realism\"\n",
    "seed = 0\n",
    "num_levers = 6  # Arbitrary - feel free to experiment\n",
    "bandit = lab0.BernoulliBandit(arms=num_levers, seed=seed)\n",
    "\n",
    "# Define selector\n",
    "rand_select = RandomSelector(K=num_levers)\n",
    "\n",
    "# Create list of selectors to compare: here just the single Random one above\n",
    "#\n",
    "selectors = [rand_select]\n",
    "\n",
    "# Pass bandit and selector to plot_performance() for assessment\n",
    "# - average over 100 repetitions\n",
    "#\n",
    "plot_performance(bandit, selectors, N=100, Reps=100, \n",
    "                 title='Purely Random Lever Selection', legend=['Random'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Slightly Smarter Algorithm: Some Exploration + Exploitation\n",
    "The next piece of code defines a simple *stateful* lever selector. It tracks the average payout for each arm, choosing the highest average (exploiting) but then occasionally, with probability set by the argument `prob`, making an entirely random choice (exploring). This selector class also includes functionality for \"annealing\" the exploration probability (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestMeanPlusSomeRandom(lab0.Selector):\n",
    "    \n",
    "    def __init__(self, K, prob=0.2, decay=1.0):\n",
    "        super().__init__(K)\n",
    "        self._prob_explore_init = prob\n",
    "        self._decay = decay\n",
    "        # State: will be initialised in reset_state()\n",
    "        self._prob_explore = None\n",
    "        self._Totals = None\n",
    "        self._Counts = None\n",
    "        self._Means = None\n",
    "        self.reset_state()\n",
    "        \n",
    "    def reset_state(self):\n",
    "        # Reset back to start (allows repeated runs)\n",
    "        self._Totals = np.zeros(self.K)\n",
    "        self._Counts = np.zeros(self.K)\n",
    "        self._Means = np.zeros(self.K)\n",
    "        self._prob_explore = self._prob_explore_init\n",
    "        \n",
    "    def update_state(self, lever, payout):\n",
    "        # Maintain running averages\n",
    "        self._Counts[lever] += 1\n",
    "        self._Totals[lever] += payout\n",
    "        self._Means[lever] = self._Totals[lever] / self._Counts[lever]\n",
    "        # Anneal the exploration (if decay<1)\n",
    "        self._prob_explore *= self._decay\n",
    "        \n",
    "    def lever_select(self):\n",
    "        if np.random.rand()<self._prob_explore:\n",
    "            # Random choice\n",
    "            lever = np.random.randint(self.K)\n",
    "        else:\n",
    "            # Highest mean\n",
    "            lever = np.argmax(self._Means)\n",
    "        return lever      \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the `BestMeanPlusSomeRandom` class, and compare against the purely random algorithm. We can test it with a fixed random exploration probability (0.2), and also with an \"annealed\" value. This starts off high (e.g. 1), but gradually reduces over time according to a decay rate. So early on it explores, but later, when it has a good estimate for the payout probabiliities, it will effectively only exploit (the exploration probability tends to zero). This is a rather heuristic approach, but with the right settings for the initial probability and the decay rate, this variant ought to provide the best earnings of the three. (You can of course experiment with the default parameter settings.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create list of selectors to compare\n",
    "#\n",
    "rand_select = RandomSelector(K=num_levers)\n",
    "rand_bestmean_select = BestMeanPlusSomeRandom(K=num_levers, prob=0.2)\n",
    "rand_bestmean_annealed_select = BestMeanPlusSomeRandom(K=num_levers, prob=1, decay=0.95)\n",
    "#\n",
    "selectors = [rand_select,\n",
    "             rand_bestmean_select,\n",
    "             rand_bestmean_annealed_select]\n",
    "#\n",
    "plot_performance(bandit, selectors, N=100, Reps=100,\n",
    "                 title='Selector Comparison', \n",
    "                 legend=['Random','Best mean + 20% random','Best mean + annealed random'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Exercise: Build a Bayesian Selector\n",
    "\n",
    "The exercise is to create your own Bayesian \"selector\" class(es) and try and maximise your earnings, over 100 pulls, for the `BernoulliBandit` (initially), and then the `GammaBandit` (later, a little trickier).\n",
    "\n",
    "\n",
    "## The Bernoulli Bandit\n",
    "\n",
    "The `BernoulliBandit` has $K$ levers, each of which pays out £25 with unknown probability $\\theta_k$.\n",
    "\n",
    "Suggested strategic approach:\n",
    "\n",
    "1. Write a `BayesBernoulliSelector` class where the `update_state()` method tracks the posterior distribution of the payout probability of each individual lever.  You will need to make a choice of hyperparameters $\\alpha_0$ and $\\beta_0$ for the prior (which can be identical for each lever - no need to overcomplicate!)\n",
    "\n",
    "2. Given the posterior distributions over $\\theta_k$, devise a `lever_select()` method.\n",
    "\n",
    "3. One suggested method would be:\n",
    "    - randomly sample all $\\hat\\theta_k$ from the posteriors (use `stats.beta.rvs`)\n",
    "    - choose the arm with highest random *sample* $\\hat\\theta_k$ value (which is *not* simply the highest posterior mean). Given the random sample, this will give the highest expected payout. After many pulls, these samples will of course converge on the true $\\theta_k$\n",
    "    - this approach is known as \"Thompson sampling\" (which you will probably meet elsewhere)\n",
    "\n",
    "\n",
    "4. Compare your class against the `BestMeanPlusSomeRandom` variants above. (Code is in place below to do this.)\n",
    "\n",
    "5. You might also wish to see if you can  beat the earnings of (3.) above with an alternative (heuristic) algorithm of your own devising, which may or may not be Bayesian.\n",
    "\n",
    "I will post an example `BayesBernoulliSelector` part-way through the lab (see if you can improve on it).\n",
    "\n",
    "\n",
    "## Recap\n",
    "\n",
    "(The information you need is in the lecture notes and slides, and repeated here.)\n",
    "\n",
    "Assuming you choose a conjugate prior over $\\theta_k$, which is Beta($\\alpha_0$, $\\beta_0$), your posterior for arm $k$ after $N$ pulls is Beta($\\alpha_N$, $\\beta_N$) with\n",
    "\n",
    "$$\n",
    "\\alpha_N = \\alpha_0 + n_{1k},\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\beta_N = \\beta_0 + n_{0k},\n",
    "$$\n",
    "\n",
    "having defined $n_{1k}$ as the number of observed \"successes\" (positive payouts) when pulling arm $k$ and $n_{0k}$ as the number of observed \"failures\".\n",
    "\n",
    "You will need to choose a single set of prior parameters $\\alpha_0$ and $\\beta_0$, and will need to track $K$ sets of posterior parameters $\\alpha_N$ and $\\beta_N$ within the `update_state()` method.\n",
    "\n",
    "## Insert Your Code Below ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your BayesBernoulliSelector class here\n",
    "        \n",
    "# I will post a specimen example class later in the lab (currently commented out)\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "# import cm50268_bayes_selector as bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The earlier assessment code is copied here, including the three variants of random selector.\n",
    "#\n",
    "# Add your `BayesBernoulliSelector` (and any other classes) to the testing list, and run\n",
    "# (the code defaults to averaging earnings over 100 repetitions)\n",
    "#\n",
    "\n",
    "# Re-create the bandit\n",
    "#\n",
    "bandit = lab0.BernoulliBandit(arms=num_levers, seed=seed)\n",
    "\n",
    "#\n",
    "# Create list of selectors to compare (first three as before)\n",
    "#\n",
    "rand_select = RandomSelector(K=num_levers)\n",
    "rand_bestmean_select = BestMeanPlusSomeRandom(K=num_levers, prob=0.2)\n",
    "rand_bestmean_annealed_select = BestMeanPlusSomeRandom(K=num_levers, prob=1, decay=0.95)\n",
    "\n",
    "#\n",
    "# Construct your Bayesian selector, initialised as appropriate\n",
    "#\n",
    "bayes_select = #### YOUR CODE HERE ####\n",
    "\n",
    "# List of selectors to test: you can add further examples (perhaps variants of bayes_select)\n",
    "selectors = [rand_select,\n",
    "             rand_bestmean_select,\n",
    "             rand_bestmean_annealed_select,\n",
    "             bayes_select]\n",
    "#\n",
    "# Assess the performance (may take a few seconds)\n",
    "#\n",
    "plot_performance(bandit, selectors, N=100, Reps=100,\n",
    "                 title='Selector Comparison', \n",
    "                 legend=['Random','Best mean + 20% random','Best mean + annealed random', 'Your Bayes'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow-up: Repeat for the Gamma Distribution\n",
    "\n",
    "`GammaBandit` is a $K$-armed bandit where the payout from each arm is a random sample from the Gamma distribution. This distribution has a fixed \"shape\" parameter $\\alpha=5$, but each arm has its own \"rate\" parameter $\\beta_k$.\n",
    "\n",
    "You will need to consult some preferred reference here (there was a hint in the lecture...), to find out the correct conjugate prior for the rate $\\beta_k$ and the corresponding posterior distribution statistics.\n",
    "\n",
    "**Two points to note:**\n",
    "\n",
    "- You should base your selector on Thompson sampling again. Sampling from the posteriors will give you values for $\\beta_k$, which then gives you an expected payout for each lever of $\\frac{\\alpha}{\\beta_k}$ (the mean of a Gamma distribution). Given the sample, choose the lever with highest mean to again maximise expected payout.\n",
    "\n",
    "- The Gamma distribution is usually defined in terms of $\\alpha$ (its \"shape\" parameter) and $\\beta$ (its \"rate\" parameter). The`scipy.stats` module parameterises the Gamma distribution in terms of `a` ($\\alpha$) and `scale`, the latter which is $1/\\beta$.\n",
    "\n",
    "## Insert Your Code Below ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your BayesGammaSelector class here\n",
    "        \n",
    "# I will post a specimen example class later in the lab (which will have been loaded earlier)\n",
    "\n",
    "#### YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new bandit\n",
    "#\n",
    "# Gamma bandit with default alpha=5\n",
    "#\n",
    "alpha = 5\n",
    "bandit = lab0.GammaBandit(num_levers, shape=alpha)\n",
    "\n",
    "# Create list of selectors to compare\n",
    "#\n",
    "rand_select = RandomSelector(K=num_levers)\n",
    "rand_bestmean_select = BestMeanPlusSomeRandom(K=num_levers, prob=0.2)\n",
    "rand_bestmean_annealed_select = BestMeanPlusSomeRandom(K=num_levers, prob=1, decay=0.95)\n",
    "\n",
    "\n",
    "bayes_select = #### YOUR CODE HERE ####\n",
    "\n",
    "# List of selectors to test: you can add further examples\n",
    "selectors = [rand_select,\n",
    "             rand_bestmean_select,\n",
    "             rand_bestmean_annealed_select,\n",
    "             bayes_select]\n",
    "#\n",
    "plot_performance(bandit, selectors, N=100, Reps=100, \n",
    "                 title='Selector Comparison', \n",
    "                 legend=['Random','Best mean + 20% random','Best mean + annealed random', 'Your Bayes'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
