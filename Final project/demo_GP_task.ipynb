{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM50268 :: Gaussian Processes (practice example)\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Implement a numpy function that evaluates the squared exponential kernel using for loops and use it to check that the tensorflow function provided is correct. \n",
    "\n",
    "- Get the GP regression code running on a toy example with a Gaussian likelihood\n",
    "\n",
    "- Now run the code on traffic count data (the training outputs are integers) and observe what happens. Add a brief text comment highlighting what has gone wrong and why. \n",
    "\n",
    "- Implement a periodic kernel to replace the squared exponential kernel and run the regression model with the new kernel. Does the situation improve slightly? \n",
    "\n",
    "\n",
    "### If you installed the latest TensorFlow 2.0, do the following 2 lines when import:\n",
    "import tensorflow.compat.v1 as tf \n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "#### TensorFlow dtype\n",
    "Due to interations between the CPU and GPU, TensorFlow is quite strict about the difference between single and double precision floating point numbers (`tf.float32` and `tf.float64`) respectively. To be safe, when declaring constants and variables in TensorFlow I always specify the `dtype` parameter by using defining `dtype` and `dtype_convert` at the top of the file. This allows, for example:\n",
    "```python\n",
    "t_vector_of_ones = tf.ones(shape=[5], dtype=dtype)\n",
    "t_correct_precision = dtype_convert(t_not_correct_precision)\n",
    "```\n",
    "If you get tensorflow errors about `tf.float32` and `tf.float64` you probably have a constant declared using the wrong `dtype` somewhere in the computational graph.\n",
    "\n",
    "#### Inverting the covariance matrix\n",
    "Covariance matrices should be symmetric and positive definite - this means that they can be inverted using the Cholesky decomposition $K = L L^{\\mathtt{T}}$ where $L$ is a square, lower triangular matrix. There is a TensorFlow operation `t_L_matrix = tf.cholesky(t_K_matrix)` that returns the matrix $L$. \n",
    "\n",
    "Once you have the decomposed matrix, it is efficient to evaluation the matrix inversion $A = K^{-1} B$ using the special solve operation `t_A_matrix = tf.cholesky_solve(t_L_matrix, t_B_matrix)`. \n",
    "\n",
    "If you need to perform the operation $A = L^{-1} B$ there is also a special operation `t_A_matrix = tf.matrix_triangular_solve(t_L_matrix, t_B_matrix)`.\n",
    "\n",
    "Because the cholesky decomposition requires a positive definite matrix, we often add a small \"jitter\" diagonal matrix to try to improve the nummerical stability (in case the eigenvalues of the kernel become close to zero). For example we would perform the following:\n",
    "```python\n",
    "jitter = 1.0e-8\n",
    "t_L_matrix = tf.cholesky(t_K_matrix + jitter * tf.eye(tf.shape(t_K_matrix)[0], dtype=dtype)\n",
    "```\n",
    "where `tf.shape()[0]` is returning the size of the kernel matrix to build a diagonal matrix identity matrix using `tf.eye()` (and we note that we specify the `dtype` of the matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:41.609885Z",
     "start_time": "2022-04-07T18:00:38.869154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import statements..\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "dtype = tf.float64\n",
    "dtype_convert = tf.to_double\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Exponential Covariance Kernel\n",
    "\n",
    "The following tensorflow code evaluates a squared exponential covariance in **vectorised** form (more efficient computation). It is good practice to check your tensorflow code as you write it using simple (but often inefficient) numpy code where we are less likely to introduce errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:41.616398Z",
     "start_time": "2022-04-07T18:00:41.611951Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lengthscale_euclid_sq_dist(t_X, t_Z, t_lengthscale):\n",
    "    t_inv_lengthscale_squared = 1.0 / tf.square(t_lengthscale)\n",
    "    xx = t_inv_lengthscale_squared * tf.reduce_sum(t_X * t_X, axis=1, keepdims=True)\n",
    "    zz = t_inv_lengthscale_squared * tf.reduce_sum(t_Z * t_Z, axis=1, keepdims=True)\n",
    "    return xx + tf.transpose(zz) - 2.0 * t_inv_lengthscale_squared * tf.matmul(t_X, t_Z, transpose_b=True)\n",
    "\n",
    "def create_squared_exp_kernel(t_X1, t_X2, t_signal_variance, t_lengthscale):\n",
    "    dist_x1x2_sq = get_lengthscale_euclid_sq_dist(t_X1, t_X2, t_lengthscale)\n",
    "    return t_signal_variance * tf.exp(- 0.5 * dist_x1x2_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 1: Implement a function that calculates the squared exponential kernel in numpy using for loops and use it to check that the TensorFlow code operates correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:41.659371Z",
     "start_time": "2022-04-07T18:00:41.621649Z"
    }
   },
   "outputs": [],
   "source": [
    "def numpy_squared_exp_kernel(X, signal_variance, lengthscale):\n",
    "    #### **** YOUR CODE HERE **** #### \n",
    "    # Add code using for loops to generate K(X, X)\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Testing part:\n",
    "lengthscale = 2.3\n",
    "signal_variance = 1.4\n",
    "\n",
    "N = 5\n",
    "X = np.random.randn(N,1)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    t_lengthscale = tf.constant(lengthscale, dtype=dtype)\n",
    "    t_signal_variance = tf.constant(signal_variance, dtype=dtype)\n",
    "    t_X = tf.constant(X, dtype=dtype)\n",
    "    \n",
    "    t_K = create_squared_exp_kernel(t_X1=t_X, \n",
    "                                    t_X2=t_X, \n",
    "                                    t_signal_variance = t_signal_variance,\n",
    "                                    t_lengthscale=t_lengthscale)\n",
    "    \n",
    "    print('TensorFlow K =\\n', session.run(t_K))\n",
    "    \n",
    "    numpy_K = numpy_squared_exp_kernel(X=X, \n",
    "                                      signal_variance=signal_variance, \n",
    "                                      lengthscale=lengthscale)\n",
    "    \n",
    "    print('My numpy checking code K = \\n', numpy_K)\n",
    "    \n",
    "    print('shape consistency is:',numpy_K.shape==t_K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Regression Model\n",
    "\n",
    "The following is TensorFlow code that implements a regression GP model with a Gaussian likelihood (as per the lecture notes). Look through to see how the function works and correlate the code with the mathematical expressions for the marginal log likelihood and the predictive posterior code. Note that we optimise the **negative log likelihood** in TensorFlow since the optimiser **minimises** the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:41.680249Z",
     "start_time": "2022-04-07T18:00:41.661602Z"
    }
   },
   "outputs": [],
   "source": [
    "# A wrapper function to create a real number variable (-infinity, infinity)\n",
    "# with an initial value that will be optimised by tensorflow.\n",
    "def create_real_variable(initial_value):\n",
    "    return tf.Variable(initial_value, dtype=dtype)\n",
    "\n",
    "# A wrapper function to create a positive variable (0, infinity) by\n",
    "# taking the exp() of a real number - this will map the real range to\n",
    "# the set of positive numbers.\n",
    "def create_positive_variable(initial_value):\n",
    "    assert initial_value > 0.0\n",
    "    return tf.exp(tf.Variable(np.log(initial_value), dtype=dtype))\n",
    "\n",
    "# A Gaussian Process class to keep all the parts of the model we need\n",
    "# grouped together\n",
    "class GP:\n",
    "    def __init__(self,\n",
    "                 description,\n",
    "                 t_objective,\n",
    "                 create_prediction_function,\n",
    "                 t_prediction_placeholder,\n",
    "                 t_prediction_mean,\n",
    "                 t_prediction_var,\n",
    "                 t_input=None,\n",
    "                 t_output=None,\n",
    "                 hyperparameter_dict={}):\n",
    "\n",
    "        self._description = description\n",
    "        self._t_objective = t_objective\n",
    "        self._create_prediction_function = create_prediction_function\n",
    "        self._t_prediction_placeholder = t_prediction_placeholder\n",
    "        self._t_prediction_mean = t_prediction_mean\n",
    "        self._t_prediction_var = t_prediction_var\n",
    "        self._t_input = t_input\n",
    "        self._t_output = t_output\n",
    "        self._hyperparameter_dict = hyperparameter_dict\n",
    "\n",
    "    @property\n",
    "    def description(self): return self._description\n",
    "\n",
    "    @property\n",
    "    def t_objective(self): return self._t_objective\n",
    "\n",
    "    @property\n",
    "    def t_prediction_placeholder(self): return self._t_prediction_placeholder\n",
    "\n",
    "    @property\n",
    "    def t_prediction_mean(self): return self._t_prediction_mean\n",
    "\n",
    "    @property\n",
    "    def t_prediction_var(self): return self._t_prediction_var\n",
    "\n",
    "    @property\n",
    "    def t_input(self): return self._t_input\n",
    "\n",
    "    @property\n",
    "    def t_output(self): return self._t_output\n",
    "\n",
    "    @property\n",
    "    def hyperparameter_dict(self): return self._hyperparameter_dict\n",
    "\n",
    "    def create_prediction(self, tf_input):\n",
    "        return self._create_prediction_function(tf_input)\n",
    "    \n",
    "\n",
    "# Create a GP regression model by specifying the input and output data, as (N x 1) matrices,\n",
    "# and the initial values for the hyperparameters (these will be turned into tensorflow variables\n",
    "# and optimised).\n",
    "def create_gp(input_data, \n",
    "              output_data, \n",
    "              initial_signal_variance,\n",
    "              initial_lengthscale,\n",
    "              initial_sigma_noise_sq):\n",
    "\n",
    "    # Constant training data in tensorflow\n",
    "    t_X = tf.constant(input_data, dtype=dtype)\n",
    "    t_Y = tf.constant(output_data, dtype=dtype)\n",
    "\n",
    "    t_N = tf.shape(t_Y)[0]\n",
    "    t_D = tf.shape(t_Y)[1]\n",
    "    t_Q = tf.shape(t_X)[1]\n",
    "    jitter = 1.0e-8\n",
    "\n",
    "    # Create variables for hyperparamers\n",
    "    t_signal_variance = create_positive_variable(initial_signal_variance)\n",
    "    t_lengthscale     = create_positive_variable(initial_lengthscale)\n",
    "    t_sigma_noise_sq  = create_positive_variable(initial_sigma_noise_sq)\n",
    "    \n",
    "    hyperparameter_dict = {'signal_variance': t_signal_variance,\n",
    "                           'lengthscale': t_lengthscale,\n",
    "                           'sigma_noise_squared': t_sigma_noise_sq}\n",
    "    \n",
    "    # Create a lambda function to ensure we use the same hyperparameters\n",
    "    # when be create the different kernels..\n",
    "    kernel_creation_function = lambda t_X1, t_X2: \\\n",
    "        create_squared_exp_kernel(t_X1=t_X1, \n",
    "                                  t_X2=t_X2,\n",
    "                                  t_lengthscale=t_lengthscale,\n",
    "                                  t_signal_variance=t_signal_variance)\n",
    "\n",
    "    # Create the training data covariance matrix + noise identity matrix\n",
    "    #\n",
    "    # NOTE: We include the jitter term to improve nummerical stability\n",
    "    #\n",
    "    t_K_xx = kernel_creation_function(t_X1=t_X, t_X2=t_X) \\\n",
    "        + (t_sigma_noise_sq + jitter) * tf.eye(t_N, dtype=dtype)\n",
    "\n",
    "    # Take the cholesky decomposition of K = L L^T to make it easy to calculate \n",
    "    # the inverse of K and the log determinant of K. This relies on K being a \n",
    "    # positive definite covariance matrix.\n",
    "    t_L_xx = tf.cholesky(t_K_xx)\n",
    "\n",
    "    # The log determinant of K is twice the sum of the log of the diagonal of the\n",
    "    # cholesky matrix\n",
    "    t_log_det = 2.0 * tf.reduce_sum(tf.log(tf.diag_part(t_L_xx)))\n",
    "\n",
    "    # We can calculate the data fit term by using the cholesky matrix L to\n",
    "    # invert the covariance matrix efficiently using the matrix triangular solve\n",
    "    # operation. \n",
    "    #\n",
    "    # Tr[K^-1 Y Y^T] = Tr[Y^T (L L^T)^-1 Y] = Tr[(Y^T L^-T) (L^-1 Y)] = sum((L^-1 Y)^2)\n",
    "    #\n",
    "    t_Kinv_YYtranspose = 0.5 * tf.reduce_sum(tf.square(\n",
    "        tf.matrix_triangular_solve(t_L_xx, t_Y, lower=True)))\n",
    "\n",
    "    # Add a loose prior on the noise variance\n",
    "    t_noise_prior = + 0.5 * tf.square(tf.log(t_sigma_noise_sq))\n",
    "    \n",
    "    # Add loose priors on the kernel hyperparameters\n",
    "    t_hyper_prior = + 0.5 * tf.square(tf.log(t_signal_variance)) \\\n",
    "                    + 0.5 * tf.square(tf.log(t_lengthscale)) \n",
    "\n",
    "    half_log_two_pi = tf.constant(0.5 * np.log(2.0 * np.pi), dtype=dtype)\n",
    "\n",
    "    # Evaluate the NEGATIVE (since we will minimise) marginal log likelihood as\n",
    "    # the objective for training the hyperparameters\n",
    "    t_neg_log_likelihood = half_log_two_pi * dtype_convert(t_D) * dtype_convert(t_N) \\\n",
    "                         + 0.5 * dtype_convert(t_D) * t_log_det \\\n",
    "                         + t_Kinv_YYtranspose + t_hyper_prior + t_noise_prior\n",
    "\n",
    "    def create_prediction(tf_input):\n",
    "        # Invert using the cholesky decomposition of the kernel\n",
    "        t_Kinv_Y = tf.cholesky_solve(t_L_xx, t_Y)\n",
    "        \n",
    "        # Calculate the kernel from the input to the training data\n",
    "        t_K_x_X = kernel_creation_function(t_X1=tf_input, t_X2=t_X)\n",
    "        \n",
    "        # Get the predicitive mean\n",
    "        t_y_mean = tf.matmul(t_K_x_X, t_Kinv_Y)\n",
    "        \n",
    "        t_K_x_x_diag = tf.diag_part(kernel_creation_function(t_X1=tf_input, t_X2=tf_input)) \\\n",
    "            + t_sigma_noise_sq * tf.ones([tf.shape(tf_input)[0]], dtype=dtype)\n",
    "\n",
    "        # Get the predicitve variance\n",
    "        t_y_var = t_K_x_x_diag - tf.reduce_sum(tf.square(\n",
    "            tf.matrix_triangular_solve(t_L_xx, tf.transpose(t_K_x_X))), axis=0)\n",
    "        \n",
    "        # Make sure a N* x 1 matrix (where N* = number of test inputs)\n",
    "        t_y_var = t_y_var[:, tf.newaxis]\n",
    "\n",
    "        # Return the predicitve mean and variance\n",
    "        return t_y_mean, t_y_var\n",
    "\n",
    "    # A placeholder for providing different test data after training. See the introduction\n",
    "    # to tensorflow for details on how the placeholders work\n",
    "    t_prediction_placeholder = tf.placeholder(dtype=dtype)\n",
    "    t_prediction_mean, t_prediction_var = create_prediction(t_prediction_placeholder)\n",
    "\n",
    "    gp = GP(description='Gaussian Process',\n",
    "            t_objective=t_neg_log_likelihood,\n",
    "            create_prediction_function=create_prediction,\n",
    "            t_prediction_placeholder=t_prediction_placeholder,\n",
    "            t_prediction_mean=t_prediction_mean,\n",
    "            t_prediction_var=t_prediction_var,\n",
    "            t_input=t_X,\n",
    "            t_output=t_Y,\n",
    "            hyperparameter_dict=hyperparameter_dict)\n",
    "\n",
    "    return gp\n",
    "\n",
    "\n",
    "# Print out the current values of the hyperparameters\n",
    "def print_hyperparameters(gp, session):\n",
    "    for (k, v) in gp.hyperparameter_dict.items():\n",
    "        print('{} = {:.6}'.format(k, session.run(v)))\n",
    "    print('')\n",
    "\n",
    "\n",
    "# Plot the predicitve posterior for the provide input values (num_test_points x 1 matrix) \n",
    "def plot_gp_predictions(gp, session, input_values):\n",
    "    # Ensure N* x 1 input variable\n",
    "    if input_values.ndim == 1:\n",
    "        input_values = input_values[:,np.newaxis]\n",
    "    \n",
    "    feed_dict = {gp.t_prediction_placeholder: input_values}\n",
    "    \n",
    "    pred_mean, pred_var = session.run((gp.t_prediction_mean, \n",
    "                                       gp.t_prediction_var), \n",
    "                                      feed_dict=feed_dict)\n",
    "    \n",
    "    plt.figure(figsize=[12,6])\n",
    "    plt.plot(session.run(gp.t_input), session.run(gp.t_output), 'k.')\n",
    "    plt.plot(input_values, pred_mean, 'r-')\n",
    "    plt.plot(input_values, pred_mean + 2.0 * np.sqrt(pred_var), 'r:')\n",
    "    plt.plot(input_values, pred_mean - 2.0 * np.sqrt(pred_var), 'r:')\n",
    "    plt.grid(True)\n",
    "    return pred_mean, pred_var\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 1a: Run the GP model on the following toy dataset to ensure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:42.857649Z",
     "start_time": "2022-04-07T18:00:41.682265Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_data = np.random.randn(N,1)\n",
    "output_data = np.sin(np.pi * input_data) + 0.05 * np.random.randn(N,1)\n",
    "\n",
    "gp = create_gp(input_data=input_data, \n",
    "               output_data=output_data, \n",
    "               initial_signal_variance=1.0,\n",
    "               initial_lengthscale=1.0,\n",
    "               initial_sigma_noise_sq=1.0)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    feed_dict = {}\n",
    "\n",
    "    t_objective = gp.t_objective\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 2000\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(t_objective)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Before Optimisation:')\n",
    "    print_hyperparameters(gp, session)\n",
    "\n",
    "    refresh_iter = int(np.ceil(num_iterations / 10))\n",
    "    for i in range(num_iterations):\n",
    "        opt, cost = session.run((optimizer, t_objective),\n",
    "                                feed_dict=feed_dict)\n",
    "        if (i % refresh_iter) == 0:\n",
    "            print('  opt iter {:5}: objective = {}'.format(i, cost))\n",
    "\n",
    "    print('Final iter {:5}: objective = {}\\n'.format(i, cost))\n",
    "    \n",
    "    print('After Optimisation:')\n",
    "    print_hyperparameters(gp, session)\n",
    "    \n",
    "    input_values = np.linspace(np.min(input_data), np.max(input_data), 200)\n",
    "    plot_gp_predictions(gp, session, input_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bike Traffic Count Dataset\n",
    "\n",
    "The following data is taken from the UK Department of Transport figures counting how many cyclists crossed Vauxhall Bridge over Monday to Friday (week starting 31st July 2008). We note that the counts per hour are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:43.056352Z",
     "start_time": "2022-04-07T18:00:42.859795Z"
    }
   },
   "outputs": [],
   "source": [
    "bike_data = pd.read_csv('traffic_data_week_2008_07_31.csv')\n",
    "bike_data = bike_data.drop([5,6])\n",
    "counts = bike_data.values.flatten()\n",
    "times = np.zeros(bike_data.shape)\n",
    "for n, r in bike_data.iterrows():\n",
    "    times[n,:] = bike_data.columns.astype(np.float64) + (n*24)\n",
    "times = times.flatten()\n",
    "\n",
    "bike_counts = counts[:,np.newaxis]\n",
    "bike_times = times[:,np.newaxis]\n",
    "\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.plot(bike_times, bike_counts, 'ko-')\n",
    "plt.grid(True)\n",
    "plt.title('Bike Traffic Data')\n",
    "plt.xlabel('Hour since start of week')\n",
    "plt.ylabel('Number of cyclists in the hour')\n",
    "\n",
    "bike_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 2: Run the GP code with a squared exponential kernel on the bike count data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:44.945149Z",
     "start_time": "2022-04-07T18:00:43.061463Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_data = bike_times\n",
    "output_data = bike_counts\n",
    "\n",
    "gp = create_gp(input_data=input_data, \n",
    "               output_data=output_data, \n",
    "               initial_signal_variance=1.0,\n",
    "               initial_lengthscale=1.0,\n",
    "               initial_sigma_noise_sq=1.0)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    feed_dict = {}\n",
    "\n",
    "    t_objective = gp.t_objective\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 2000\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(t_objective)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print('Before Optimisation:')\n",
    "    print_hyperparameters(gp, session)\n",
    "\n",
    "    refresh_iter = int(np.ceil(num_iterations / 10))\n",
    "    for i in range(num_iterations):\n",
    "        opt, cost = session.run((optimizer, t_objective),\n",
    "                                feed_dict=feed_dict)\n",
    "        if (i % refresh_iter) == 0:\n",
    "            print('  opt iter {:5}: objective = {}'.format(i, cost))\n",
    "\n",
    "    print('Final iter {:5}: objective = {}\\n'.format(i, cost))\n",
    "    \n",
    "    print('After Optimisation:')\n",
    "    print_hyperparameters(gp, session)\n",
    "    \n",
    "    input_values = np.linspace(np.min(input_data), np.max(input_data), 200)\n",
    "    plot_gp_predictions(gp, session, input_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to using periodic kernel\n",
    "\n",
    "###  Task 3: Implement the periodic kernel and copy and paste the GP code from above, modifying it to use your new periodic kernel and verify the results by running the code with pre-defined settings.\n",
    "\n",
    "**The periodic kernel is defined as:**\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\sigma_{\\mathrm{f}}^{2} \\exp\\!\\left( - 2 \\gamma \\left[\\sin\\left(\\frac{\\pi}{T} \\sqrt{\\|x_i - x_j\\|^2} \\right)\\right]^2 \\right)\n",
    "$$\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Signal variance $\\sigma_{\\mathrm{f}}^2$ (positive)\n",
    "- Inverse Lengthscale $\\gamma = 1 / \\ell^2$ (positive)\n",
    "- Period $T$ (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:44.958208Z",
     "start_time": "2022-04-07T18:00:44.947980Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_periodic_kernel(t_X1, t_X2, t_signal_variance, t_gamma, t_period):\n",
    "    #### **** YOUR CODE HERE **** #### \n",
    "    # Add code to this function to generate tf.Tensor K(X,X)..create_periodic_kernel\n",
    "    \n",
    "    \n",
    "    return K   \n",
    "„ÄÅ\n",
    "\n",
    "#### **** YOUR CODE BELOW **** #### \n",
    "# Modify the below function as appropriate..\n",
    "def create_periodic_gp(input_data, \n",
    "                       output_data, \n",
    "                       initial_signal_variance,\n",
    "                       initial_gamma,\n",
    "                       initial_period,\n",
    "                       initial_sigma_noise_sq):\n",
    "\n",
    "    # Constant training data in tensorflow\n",
    "    t_X    = tf.constant(input_data, dtype=dtype)\n",
    "    t_Y    = tf.constant(output_data, dtype=dtype)\n",
    "\n",
    "    t_N    = tf.shape(t_Y)[0]\n",
    "    t_D    = tf.shape(t_Y)[1]\n",
    "    t_Q    = tf.shape(t_X)[1]\n",
    "    jitter = 1.0e-8\n",
    "\n",
    "    # Create variables for hyperparamers\n",
    "    t_signal_variance   = create_positive_variable(initial_signal_variance)\n",
    "    t_sigma_noise_sq    = create_positive_variable(initial_sigma_noise_sq)\n",
    "    t_period            = create_positive_variable(initial_period)\n",
    "    t_gamma             = create_positive_variable(initial_gamma)\n",
    "    \n",
    "    hyperparameter_dict = {'signal_variance': t_signal_variance,\n",
    "                           'period': t_period,\n",
    "                           'gamma': t_gamma,\n",
    "                           'sigma_noise_squared': t_sigma_noise_sq}\n",
    "    \n",
    "    # Create a lambda function to ensure we use the same hyperparameters\n",
    "    # when be create the different kernels..\n",
    "    kernel_creation_function = lambda t_X1, t_X2: \\\n",
    "        create_periodic_kernel(t_X1        = t_X1, \n",
    "                                  t_X2     = t_X2,\n",
    "                                  t_gamma  = t_gamma,\n",
    "                                  t_period = t_period,\n",
    "                                  t_signal_variance=t_signal_variance)\n",
    "\n",
    "    # Create the training data covariance matrix + noise identity matrix\n",
    "    #\n",
    "    # NOTE: We include the jitter term to improve nummerical stability\n",
    "    #\n",
    "    t_K_xx = kernel_creation_function(t_X1=t_X, t_X2=t_X) \\\n",
    "        + (t_sigma_noise_sq + jitter) * tf.eye(t_N, dtype=dtype)\n",
    "\n",
    "    # Take the cholesky decomposition of K = L L^T to make it easy to calculate \n",
    "    # the inverse of K and the log determinant of K. This relies on K being a \n",
    "    # positive definite covariance matrix.\n",
    "    t_L_xx = tf.cholesky(t_K_xx)\n",
    "\n",
    "    # The log determinant of K is twice the sum of the log of the diagonal of the\n",
    "    # cholesky matrix\n",
    "    t_log_det = 2.0 * tf.reduce_sum(tf.log(tf.diag_part(t_L_xx)))\n",
    "\n",
    "    # We can calculate the data fit term by using the cholesky matrix L to\n",
    "    # invert the covariance matrix efficiently using the matrix triangular solve\n",
    "    # operation. \n",
    "    #\n",
    "    # Tr[K^-1 Y Y^T] = Tr[Y^T (L L^T)^-1 Y] = Tr[(Y^T L^-T) (L^-1 Y)] = sum((L^-1 Y)^2)\n",
    "    #\n",
    "    t_Kinv_YYtranspose = 0.5 * tf.reduce_sum(tf.square(\n",
    "        tf.matrix_triangular_solve(t_L_xx, t_Y, lower=True)))\n",
    "\n",
    "    # Add a loose prior on the noise variance\n",
    "    t_noise_prior = + 0.5 * tf.square(tf.log(t_sigma_noise_sq))\n",
    "    \n",
    "    # Add loose priors on the kernel hyperparameters\n",
    "    t_hyper_prior = + 0.5 * tf.square(tf.log(t_signal_variance)) \\\n",
    "                    + 0.5 * tf.square(tf.log(t_gamma)) \n",
    "\n",
    "    half_log_two_pi = tf.constant(0.5 * np.log(2.0 * np.pi), dtype=dtype)\n",
    "\n",
    "    # Evaluate the NEGATIVE (since we will minimise) marginal log likelihood as\n",
    "    # the objective for training the hyperparameters\n",
    "    t_neg_log_likelihood = half_log_two_pi * dtype_convert(t_D) * dtype_convert(t_N) \\\n",
    "                         + 0.5 * dtype_convert(t_D) * t_log_det \\\n",
    "                         + t_Kinv_YYtranspose + t_hyper_prior + t_noise_prior\n",
    "\n",
    "    def create_prediction(tf_input):\n",
    "        # Invert using the cholesky decomposition of the kernel\n",
    "        t_Kinv_Y = tf.cholesky_solve(t_L_xx, t_Y)\n",
    "        \n",
    "        # Calculate the kernel from the input to the training data\n",
    "        t_K_x_X = kernel_creation_function(t_X1=tf_input, t_X2=t_X)\n",
    "        \n",
    "        # Get the predicitive mean\n",
    "        t_y_mean = tf.matmul(t_K_x_X, t_Kinv_Y)\n",
    "        \n",
    "        t_K_x_x_diag = tf.diag_part(kernel_creation_function(t_X1=tf_input, t_X2=tf_input)) \\\n",
    "            + t_sigma_noise_sq * tf.ones([tf.shape(tf_input)[0]], dtype=dtype)\n",
    "\n",
    "        # Get the predicitve variance\n",
    "        t_y_var = t_K_x_x_diag - tf.reduce_sum(tf.square(\n",
    "            tf.matrix_triangular_solve(t_L_xx, tf.transpose(t_K_x_X))), axis=0)\n",
    "        \n",
    "        # Make sure a N* x 1 matrix (where N* = number of test inputs)\n",
    "        t_y_var = t_y_var[:, tf.newaxis]\n",
    "\n",
    "        # Return the predicitve mean and variance\n",
    "        return t_y_mean, t_y_var\n",
    "\n",
    "    # A placeholder for providing different test data after training. See the introduction\n",
    "    # to tensorflow for details on how the placeholders work\n",
    "    t_prediction_placeholder = tf.placeholder(dtype=dtype)\n",
    "    t_prediction_mean, t_prediction_var = create_prediction(t_prediction_placeholder)\n",
    "\n",
    "    gp = GP(description='Periodic Gaussian Process',\n",
    "            t_objective=t_neg_log_likelihood,\n",
    "            create_prediction_function=create_prediction,\n",
    "            t_prediction_placeholder=t_prediction_placeholder,\n",
    "            t_prediction_mean=t_prediction_mean,\n",
    "            t_prediction_var=t_prediction_var,\n",
    "            t_input=t_X,\n",
    "            t_output=t_Y,\n",
    "            hyperparameter_dict=hyperparameter_dict)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a *Run the code below after writing the functions above..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:47.276560Z",
     "start_time": "2022-04-07T18:00:44.959900Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_pgp_predictions(gp, session, input_values):\n",
    "    # Ensure N* x 1 input variable\n",
    "    if input_values.ndim == 1:\n",
    "        input_values = input_values[:,np.newaxis]\n",
    "    \n",
    "    feed_dict = {gp.t_prediction_placeholder: input_values}\n",
    "    \n",
    "    pred_mean, pred_var = session.run((gp.t_prediction_mean, \n",
    "                                       gp.t_prediction_var), \n",
    "                                      feed_dict=feed_dict)\n",
    "    plt.figure(figsize=[12,6])\n",
    "#     plt.plot(session.run(gp.t_input), session.run(gp.t_output), 'k.')\n",
    "    plt.plot(input_values, np.exp(pred_mean), 'r-')\n",
    "    plt.plot(input_values, np.exp(pred_mean + 2.0 * np.sqrt(pred_var)), 'r:')\n",
    "    plt.plot(input_values, np.exp(pred_mean - 2.0 * np.sqrt(pred_var)), 'r:')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return pred_mean, pred_var\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "N = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "input_data = bike_times\n",
    "output_data = bike_counts\n",
    "\n",
    "gp_periodic = create_periodic_gp(input_data   = input_data, \n",
    "                        output_data  = output_data, \n",
    "                        initial_signal_variance = 1.0,\n",
    "                        initial_gamma  = 1.0,\n",
    "                        initial_period = 24.0,\n",
    "                        initial_sigma_noise_sq = 1.0)\n",
    "\n",
    "\n",
    "\n",
    "# session = tf.Session()\n",
    "with tf.Session() as session:\n",
    "    feed_dict = {}\n",
    "\n",
    "    t_objective = gp_periodic.t_objective\n",
    "    learning_rate = 0.1\n",
    "    num_iterations = 2000\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(t_objective)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('Before Optimisation:')\n",
    "    print_hyperparameters(gp_periodic, session)\n",
    "\n",
    "    refresh_iter = int(np.ceil(num_iterations / 10))\n",
    "    for i in range(num_iterations):\n",
    "        opt, cost = session.run((optimizer, t_objective),\n",
    "                                feed_dict=feed_dict)\n",
    "        if (i % refresh_iter) == 0:\n",
    "            print('  opt iter {:5}: objective = {}'.format(i, cost))\n",
    "\n",
    "    print('Final iter {:5}: objective = {}\\n'.format(i, cost))\n",
    "\n",
    "    print('After Optimisation:')\n",
    "    print_hyperparameters(gp_periodic, session)\n",
    "\n",
    "    input_values = np.linspace(np.min(input_data), np.max(input_data), 200)\n",
    "\n",
    "    periodic_mean, periodic_var = plot_gp_predictions(gp_periodic, session, input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative way to implement GP for the same problem using the sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T18:00:48.337546Z",
     "start_time": "2022-04-07T18:00:47.278545Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared, ConstantKernel\n",
    "X= bike_times\n",
    "y = bike_counts\n",
    "k0 = WhiteKernel(noise_level=0.3**2, noise_level_bounds=(0.1**2, 0.5**2))\n",
    "\n",
    "k1 = ConstantKernel(constant_value=2) * \\\n",
    "  ExpSineSquared(length_scale=1.0, periodicity=21, periodicity_bounds=(15, 30))\n",
    "\n",
    "kernel_1  = k0 + k1 \n",
    "gp1 = gaussian_process.GaussianProcessRegressor(\n",
    "    kernel=kernel_1, \n",
    "    n_restarts_optimizer=10, \n",
    "    normalize_y=True,\n",
    "    alpha=0.0\n",
    ")\n",
    "gp1.fit(X[:50], y[:50]) # 50 samples as training set\n",
    "\n",
    "pred_mean, pred_var = gp1.predict(X, return_std=True) # all samples as test+training here, just for visualization\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.plot(X, pred_mean, 'r-')\n",
    "plt.plot(X, pred_mean + 2.0 * np.sqrt(pred_var), 'r:')\n",
    "plt.plot(X, pred_mean - 2.0 *  np.sqrt(pred_var), 'r:')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
